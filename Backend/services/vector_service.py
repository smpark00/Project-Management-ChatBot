import os
import json
import pandas as pd
from pathlib import Path
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

BASE_DIRECTORY = Path(
    os.path.abspath(os.path.join(os.path.dirname(__file__), "../storage"))
)
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"


class VectorStoreService:
    def __init__(
        self,
        base_directory: Path = BASE_DIRECTORY,
        embeddings_model_name: str = MODEL_NAME,
    ):
        self.base_directory = Path(base_directory).resolve()
        self.embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)

    def get_vectorstore_path(self, repo_name: str) -> Path:
        """í”„ë¡œì íŠ¸ë³„ ë²¡í„°ìŠ¤í† ì–´ê°€ ì €ì¥ë  ê²½ë¡œë¥¼ ë°˜í™˜"""
        return self.base_directory / repo_name / "vectorstore"

    def build_vector_database(self, repo_name: str) -> dict:
        """
        CSV ë°ì´í„°ì—ì„œ í•„ìš”í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„± ë° ì €ì¥
        """
        csv_dir = self.base_directory / repo_name / "csv"
        if not csv_dir.exists():
            raise FileNotFoundError(f"CSV í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {csv_dir}")

        csv_files = [
            f"{repo_name}_commits.csv",
            f"{repo_name}_issues.csv",
            f"{repo_name}_pull_requests.csv",
        ]

        df_list = []
        for filename in csv_files:
            path = csv_dir / filename
            if path.exists():
                temp_df = pd.read_csv(path)
                df_list.append(temp_df)
            else:
                print(f"âš ï¸ CSV íŒŒì¼ì´ ì—†ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤: {path}")

        if not df_list:
            raise FileNotFoundError(
                f"'{repo_name}' ê´€ë ¨ CSVê°€ í•˜ë‚˜ë„ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ({csv_dir})"
            )

        df = pd.concat(df_list, ignore_index=True)
        print(f"âœ… CSV {len(df_list)}ê°œë¥¼ í•©ì³ì„œ ì´ {len(df)}ê°œ ë ˆì½”ë“œë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤.")

        docs = []
        doc_dict = {}  # âœ… docstore.json ì €ì¥ìš©
        index_to_docstore_id = {}  # âœ… index_to_docstore_id.json ì €ì¥ìš©

        for idx, row in df.iterrows():
            if "ID" not in row:
                raise ValueError(
                    "CSVì— 'ID' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. doc_idë¥¼ ë­˜ë¡œ ì“¸ì§€ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤."
                )

            doc_id = str(row["ID"]).strip()

            # ğŸ”¹ Title, Message, Body ë“±ì˜ í•„ë“œë¥¼ í•©ì³ ë¬¸ì„œ ë‚´ìš© ìƒì„±
            content_parts = []
            if "Title" in row and pd.notna(row["Title"]):
                content_parts.append(f"Title: {row['Title']}")
            if "Message" in row and pd.notna(row["Message"]):
                content_parts.append(f"Message: {row['Message']}")
            if "Body" in row and pd.notna(row["Body"]):
                content_parts.append(f"Body: {row['Body']}")
            content = (
                "\n".join(content_parts) if content_parts else "No content available."
            )

            filename = row.get("filename", f"file_{doc_id}")

            doc = Document(
                page_content=content, metadata={"source": filename, "id": doc_id}
            )
            docs.append(doc)

            # âœ… JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ì €ì¥
            doc_dict[doc_id] = {"page_content": content}

            # âœ… FAISS ì¸ë±ìŠ¤ì™€ ë§¤í•‘ì„ ë§ì¶”ê¸° ìœ„í•´ idxë¥¼ ì‚¬ìš©
            index_to_docstore_id[str(idx)] = doc_id

        print(f"âœ… Document ê°ì²´ ìƒì„± ì™„ë£Œ. ì´ {len(docs)}ê°œ")

        # 3) FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± & ì €ì¥
        vectorstore = FAISS.from_documents(docs, self.embeddings)
        vectorstore.index_to_docstore_id = (
            index_to_docstore_id  # âœ… FAISS ê°ì²´ì— ì§ì ‘ ë§¤í•‘ ì¶”ê°€
        )

        vectorstore_path = self.get_vectorstore_path(repo_name)
        vectorstore_path.mkdir(parents=True, exist_ok=True)
        vectorstore.save_local(str(vectorstore_path))

        # âœ… index_to_docstore_id.json ì €ì¥
        index_mapping_path = vectorstore_path / "index_to_docstore_id.json"
        with open(index_mapping_path, "w", encoding="utf-8") as f:
            json.dump(index_to_docstore_id, f, ensure_ascii=False, indent=2)

        # âœ… docstore.json ì €ì¥
        docstore_path = vectorstore_path / "docstore.json"
        with open(docstore_path, "w", encoding="utf-8") as f:
            json.dump(doc_dict, f, ensure_ascii=False, indent=2)

        print(f"âœ… ë²¡í„°ìŠ¤í† ì–´ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„± ë° ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {vectorstore_path}")

        return {
            "vectorstore_directory": str(vectorstore_path),
            "docstore_file": str(docstore_path),
            "index_to_docstore_file": str(index_mapping_path),
            "doc_count": len(docs),
        }
